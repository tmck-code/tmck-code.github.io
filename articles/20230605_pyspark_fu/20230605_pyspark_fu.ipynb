{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a22b5d9",
   "metadata": {},
   "source": [
    "# Pyspark Fu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e9929",
   "metadata": {},
   "source": [
    "## 1. Initialising the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "938cae9a-282a-47ea-9828-0d082d94774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "CONF = {\n",
    "    'spark.ui.showConsoleProgress':       'false',\n",
    "    'spark.ui.dagGraph.retainedRootRDDs': '1',\n",
    "    'spark.ui.retainedJobs':              '1',\n",
    "    'spark.ui.retainedStages':            '1',\n",
    "    'spark.ui.retainedTasks':             '1',\n",
    "    'spark.sql.ui.retainedExecutions':    '1',\n",
    "    'spark.worker.ui.retainedExecutors':  '1',\n",
    "    'spark.worker.ui.retainedDrivers':    '1',\n",
    "    'spark.executor.instances':           '1',\n",
    "}\n",
    "\n",
    "def spark_session() -> SparkSession:\n",
    "    '''\n",
    "    - set a bunch of spark config variables that help lighten the load\n",
    "    - local[1] locks the spark runtime to a single core\n",
    "    - silence noisy warning logs\n",
    "    '''\n",
    "    conf = SparkConf().setAll([(k,v) for k,v in CONF.items()])\n",
    "\n",
    "    sc = SparkSession.builder.master('local[1]').config(conf=conf).getOrCreate()\n",
    "    sc.sparkContext.setLogLevel('ERROR')\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a9afc91-dafc-4e2c-98f4-ecc8e3b876ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[39m=\u001b[39m spark_session()\n",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mspark_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m- set a bunch of spark config variables that help lighten the load\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m- local[1] locks the spark runtime to a single core\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m- silence noisy warning logs\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     22\u001b[0m conf \u001b[39m=\u001b[39m SparkConf()\u001b[39m.\u001b[39msetAll([(k,v) \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m CONF\u001b[39m.\u001b[39mitems()])\n\u001b[0;32m---> 24\u001b[0m sc \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mmaster(\u001b[39m'\u001b[39;49m\u001b[39mlocal[1]\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mconfig(conf\u001b[39m=\u001b[39;49mconf)\u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[1;32m     25\u001b[0m sc\u001b[39m.\u001b[39msparkContext\u001b[39m.\u001b[39msetLogLevel(\u001b[39m'\u001b[39m\u001b[39mERROR\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m sc\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    476\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    478\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    513\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 198\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    431\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> 432\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    433\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/java_gateway.py:59\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     proc \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     SPARK_HOME \u001b[39m=\u001b[39m _find_spark_home()\n\u001b[1;32m     60\u001b[0m     \u001b[39m# Launch the Py4j gateway using Spark's run command so that we pick up the\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[39m# proper classpath and settings from spark-env.sh\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     on_windows \u001b[39m=\u001b[39m platform\u001b[39m.\u001b[39msystem() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWindows\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/find_spark_home.py:70\u001b[0m, in \u001b[0;36m_find_spark_home\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     import_error_raised \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m# Normalize the paths\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m paths \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mabspath(p) \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m paths]\n\u001b[1;32m     72\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(path \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m paths \u001b[39mif\u001b[39;00m is_spark_home(path))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/find_spark_home.py:70\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m     import_error_raised \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m# Normalize the paths\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m paths \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mabspath(p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m paths]\n\u001b[1;32m     72\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(path \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m paths \u001b[39mif\u001b[39;00m is_spark_home(path))\n",
      "File \u001b[0;32m<frozen posixpath>:404\u001b[0m, in \u001b[0;36mabspath\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "spark = spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dbdeb5",
   "metadata": {},
   "source": [
    "## 2. Create a simple dataframe for debugging\n",
    "\n",
    "\n",
    "- The pyspark official docs don't often \"create\" the dataframe that the code examples refer to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a27d70-e893-4810-92a6-7ffa43b11c15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame([\n\u001b[1;32m      2\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m}},\n\u001b[1;32m      3\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mz\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m}},\n\u001b[1;32m      4\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m}}\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      7\u001b[0m df\u001b[39m.\u001b[39mshow(truncate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    {'a': 'b', 'n': {'a': 'b'}},\n",
    "    {'a': 'c', 'n': {'z': 'x', 'y': 'b'}},\n",
    "    {'a': 'd', 'n': {'o': None, 't': 'a', '2': 3}}\n",
    "])\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0f0b4-a002-4947-b340-cb38912be8aa",
   "metadata": {},
   "source": [
    "## 3. Joins\n",
    "\n",
    "### 3.1. Avoid duplicate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54888af5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Let's construct two dataframes that share a column to join on\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df1 \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame([\n\u001b[1;32m      4\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m123\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mpikachu\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[1;32m      5\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m999\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mevee\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[1;32m      6\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m007\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcharizard\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m df2 \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame([\n\u001b[1;32m      9\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m123\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mash\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[1;32m     10\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m999\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mchloe\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[1;32m     11\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m007\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mash\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[1;32m     12\u001b[0m ])\n\u001b[1;32m     14\u001b[0m df1\u001b[39m.\u001b[39mshow(), df2\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's construct two dataframes that share a column to join on\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    {'id': '123', 'name': 'pikachu'},\n",
    "    {'id': '999', 'name': 'evee'},\n",
    "    {'id': '007', 'name': 'charizard'},\n",
    "])\n",
    "df2 = spark.createDataFrame([\n",
    "    {'id': '123', 'name': 'ash'},\n",
    "    {'id': '999', 'name': 'chloe'},\n",
    "    {'id': '007', 'name': 'ash'},\n",
    "])\n",
    "\n",
    "df1.show(), df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb73a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets join them together into a combined pokemon-and-trainer table\n",
    "joined = df1.join(\n",
    "    df2,\n",
    "    on=df1['id'] == df2['id'],\n",
    "    how='inner',\n",
    ")\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af898b40",
   "metadata": {},
   "source": [
    "This _seems_ fine initially, but spark blows up as soon as you try and use the 'id' column in an expression\n",
    "\n",
    "This example will produce the error:\n",
    "\n",
    "`[AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`].`\n",
    "\n",
    "This can be particularly annoying as the error will only appear when you attempt to use the columns, but will go undetected if this doesn't happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.utils\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import List\n",
    "\n",
    "def try_select(df: DataFrame, cols: List[str]):\n",
    "    try:\n",
    "        df.select(*cols).show()\n",
    "\n",
    "    except pyspark.sql.utils.AnalysisException as e:\n",
    "        print('select failed!', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_select(joined, ['id', 'name', 'trainer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d4744",
   "metadata": {},
   "source": [
    "The solution: use a different parameter for the `on` columns\n",
    "\n",
    "### 3.1.2 Join using list of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = df1.join(\n",
    "    df2,\n",
    "    on=['id'],\n",
    "    how='inner',\n",
    ")\n",
    "joined.show()\n",
    "\n",
    "# Now let's try that same select again\n",
    "try_select(joined, ['id', 'name', 'trainer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414bf5ac",
   "metadata": {},
   "source": [
    "### 3.1.3 Dataframe aliasing is a bit weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.alias('pokemon').select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "joined = df1.alias('pokemon').join(\n",
    "    df2.alias('trainers'),\n",
    "    on=F.col('pokemon.id') == F.col('trainers.id'),\n",
    "    how='inner',\n",
    ")\n",
    "joined.show()\n",
    "joined.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19620ae6",
   "metadata": {},
   "source": [
    "Now, our error message is much better, as it contains the dataframe aliases identifying which table the duplicate column name is from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a82b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_select(joined, ['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d393943",
   "metadata": {},
   "source": [
    "Confusingly, using `Dataframe.columns` does not show the aliases, but they are usable when selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be334bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joined.columns)\n",
    "\n",
    "try_select(joined, ['pokemon.id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f751804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
