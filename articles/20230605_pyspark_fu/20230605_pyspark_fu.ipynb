{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a22b5d9",
   "metadata": {},
   "source": [
    "# Pyspark Fu\n",
    "\n",
    "- [Initialising the Spark Session](#initialising-the-spark-session)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e2e9929",
   "metadata": {},
   "source": [
    "## Initialising the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938cae9a-282a-47ea-9828-0d082d94774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "CONF = {\n",
    "    'spark.ui.showConsoleProgress':       'false',\n",
    "    'spark.ui.dagGraph.retainedRootRDDs': '1',\n",
    "    'spark.ui.retainedJobs':              '1',\n",
    "    'spark.ui.retainedStages':            '1',\n",
    "    'spark.ui.retainedTasks':             '1',\n",
    "    'spark.sql.ui.retainedExecutions':    '1',\n",
    "    'spark.worker.ui.retainedExecutors':  '1',\n",
    "    'spark.worker.ui.retainedDrivers':    '1',\n",
    "    'spark.executor.instances':           '1',\n",
    "}\n",
    "\n",
    "def spark_session() -> SparkSession:\n",
    "    '''\n",
    "    - set a bunch of spark config variables that help lighten the load\n",
    "    - local[1] locks the spark runtime to a single core\n",
    "    - silence noisy warning logs\n",
    "    '''\n",
    "    conf = SparkConf().setAll([(k,v) for k,v in CONF.items()])\n",
    "\n",
    "    sc = SparkSession.builder.master('local[1]').config(conf=conf).getOrCreate()\n",
    "    sc.sparkContext.setLogLevel('ERROR')\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9afc91-dafc-4e2c-98f4-ecc8e3b876ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/04 08:36:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a27d70-e893-4810-92a6-7ffa43b11c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------+\n",
      "|a  |n                          |\n",
      "+---+---------------------------+\n",
      "|b  |{a -> b}                   |\n",
      "|c  |{y -> b, z -> x}           |\n",
      "|d  |{2 -> 3, t -> a, o -> null}|\n",
      "+---+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    {'a': 'b', 'n': {'a': 'b'}},\n",
    "    {'a': 'c', 'n': {'z': 'x', 'y': 'b'}},\n",
    "    {'a': 'd', 'n': {'o': None, 't': 'a', '2': 3}}\n",
    "])\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05f0f0b4-a002-4947-b340-cb38912be8aa",
   "metadata": {},
   "source": [
    "## Avoid duplicate column names when joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54888af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|123|  pikachu|\n",
      "|999|     evee|\n",
      "|007|charizard|\n",
      "+---+---------+\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|123|  ash|\n",
      "|999|chloe|\n",
      "|007|  ash|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's construct two dataframes that share a column to join on\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    {'id': '123', 'name': 'pikachu'},\n",
    "    {'id': '999', 'name': 'evee'},\n",
    "    {'id': '007', 'name': 'charizard'},\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    {'id': '123', 'name': 'ash'},\n",
    "    {'id': '999', 'name': 'chloe'},\n",
    "    {'id': '007', 'name': 'ash'},\n",
    "])\n",
    "\n",
    "df1.show(), df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6eb73a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+-------+\n",
      "| id|     name| id|trainer|\n",
      "+---+---------+---+-------+\n",
      "|007|charizard|007|    ash|\n",
      "|123|  pikachu|123|    ash|\n",
      "|999|     evee|999|  chloe|\n",
      "+---+---------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, lets join them together into a combined pokemon-and-trainer table\n",
    "joined = df1.join(\n",
    "    df2,\n",
    "    on=df1['id'] == df2['id'],\n",
    "    how='inner',\n",
    ")\n",
    "joined.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af898b40",
   "metadata": {},
   "source": [
    "This _seems_ fine initially, but spark blows up as soon as you try and use the 'id' column in an expression\n",
    "\n",
    "This example will produce the error:\n",
    "\n",
    "`[AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`].`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5cab4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.utils\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import List\n",
    "\n",
    "def try_select(df: DataFrame, cols: List[str]):\n",
    "    try:\n",
    "        df.select(*cols).show()\n",
    "\n",
    "    except pyspark.sql.utils.AnalysisException as e:\n",
    "        print('select failed!', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34f0c2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select failed! [AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`].\n"
     ]
    }
   ],
   "source": [
    "try_select(joined, ['id', 'name', 'trainer'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "012d4744",
   "metadata": {},
   "source": [
    "There are two techniques to mitigate this that I've found:\n",
    "\n",
    "- using a different parameter for the `on` columns\n",
    "- dataframe aliasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0bc54b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+\n",
      "| id|     name|trainer|\n",
      "+---+---------+-------+\n",
      "|007|charizard|    ash|\n",
      "|123|  pikachu|    ash|\n",
      "|999|     evee|  chloe|\n",
      "+---+---------+-------+\n",
      "\n",
      "+---+---------+-------+\n",
      "| id|     name|trainer|\n",
      "+---+---------+-------+\n",
      "|007|charizard|    ash|\n",
      "|123|  pikachu|    ash|\n",
      "|999|     evee|  chloe|\n",
      "+---+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = df1.join(\n",
    "    df2,\n",
    "    on=['id'],\n",
    "    how='inner',\n",
    ")\n",
    "joined.show()\n",
    "\n",
    "# Now let's try that same select again\n",
    "try_select(joined, ['id', 'name', 'trainer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b46a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|123|  pikachu|\n",
      "|999|     evee|\n",
      "|007|charizard|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.alias('pokemon').select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccae01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+-----+\n",
      "| id|     name| id| name|\n",
      "+---+---------+---+-----+\n",
      "|007|charizard|007|  ash|\n",
      "|123|  pikachu|123|  ash|\n",
      "|999|     evee|999|chloe|\n",
      "+---+---------+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'id', 'name']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "joined = df1.alias('pokemon').join(\n",
    "    df2.alias('trainers'),\n",
    "    on=F.col('pokemon.id') == F.col('trainers.id'),\n",
    "    how='inner',\n",
    ")\n",
    "joined.show()\n",
    "joined.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19620ae6",
   "metadata": {},
   "source": [
    "Now, our error message is much better, as it contains the dataframe aliases identifying which table the duplicate column name is from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d0a82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select failed! [AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`pokemon`.`id`, `trainers`.`id`].\n"
     ]
    }
   ],
   "source": [
    "try_select(joined, ['id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d393943",
   "metadata": {},
   "source": [
    "Confusingly, using `Dataframe.columns` does not show the aliases, but they are usable when selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3be334bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'id', 'name']\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|007|\n",
      "|123|\n",
      "|999|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(joined.columns)\n",
    "\n",
    "try_select(joined, ['pokemon.id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
