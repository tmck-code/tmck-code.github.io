{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a22b5d9",
   "metadata": {},
   "source": [
    "# Pyspark Fu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e9929",
   "metadata": {},
   "source": [
    "## 1. Initialising the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "938cae9a-282a-47ea-9828-0d082d94774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "CONF = {\n",
    "    'spark.ui.showConsoleProgress':       'false',\n",
    "    'spark.ui.dagGraph.retainedRootRDDs': '1',\n",
    "    'spark.ui.retainedJobs':              '1',\n",
    "    'spark.ui.retainedStages':            '1',\n",
    "    'spark.ui.retainedTasks':             '1',\n",
    "    'spark.sql.ui.retainedExecutions':    '1',\n",
    "    'spark.worker.ui.retainedExecutors':  '1',\n",
    "    'spark.worker.ui.retainedDrivers':    '1',\n",
    "    'spark.executor.instances':           '1',\n",
    "}\n",
    "\n",
    "def spark_session() -> SparkSession:\n",
    "    '''\n",
    "    - set a bunch of spark config variables that help lighten the load\n",
    "    - local[1] locks the spark runtime to a single core\n",
    "    - silence noisy warning logs\n",
    "    '''\n",
    "    conf = SparkConf().setAll([(k,v) for k,v in CONF.items()])\n",
    "\n",
    "    sc = SparkSession.builder.master('local[1]').config(conf=conf).getOrCreate()\n",
    "    sc.sparkContext.setLogLevel('ERROR')\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9afc91-dafc-4e2c-98f4-ecc8e3b876ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dbdeb5",
   "metadata": {},
   "source": [
    "## 2. Create a simple dataframe for debugging\n",
    "\n",
    "\n",
    "- The pyspark official docs don't often \"create\" the dataframe that the code examples refer to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a27d70-e893-4810-92a6-7ffa43b11c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------+\n",
      "|a  |n                          |\n",
      "+---+---------------------------+\n",
      "|b  |{a -> b}                   |\n",
      "|c  |{y -> b, z -> x}           |\n",
      "|d  |{2 -> 3, t -> a, o -> null}|\n",
      "+---+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    {'a': 'b', 'n': {'a': 'b'}},\n",
    "    {'a': 'c', 'n': {'z': 'x', 'y': 'b'}},\n",
    "    {'a': 'd', 'n': {'o': None, 't': 'a', '2': 3}}\n",
    "])\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0f0b4-a002-4947-b340-cb38912be8aa",
   "metadata": {},
   "source": [
    "## 3. Joins\n",
    "\n",
    "### 3.1. Avoid duplicate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54888af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|123|  pikachu|\n",
      "|999|     evee|\n",
      "|007|charizard|\n",
      "+---+---------+\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|123|  ash|\n",
      "|999|chloe|\n",
      "|007|  ash|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's construct two dataframes that share a column to join on\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    {'id': '123', 'name': 'pikachu'},\n",
    "    {'id': '999', 'name': 'evee'},\n",
    "    {'id': '007', 'name': 'charizard'},\n",
    "])\n",
    "df2 = spark.createDataFrame([\n",
    "    {'id': '123', 'name': 'ash'},\n",
    "    {'id': '999', 'name': 'chloe'},\n",
    "    {'id': '007', 'name': 'ash'},\n",
    "])\n",
    "\n",
    "df1.show(), df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb73a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+-----+\n",
      "| id|     name| id| name|\n",
      "+---+---------+---+-----+\n",
      "|007|charizard|007|  ash|\n",
      "|123|  pikachu|123|  ash|\n",
      "|999|     evee|999|chloe|\n",
      "+---+---------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, lets join them together into a combined pokemon-and-trainer table\n",
    "joined = df1.join(\n",
    "    df2,\n",
    "    on=df1['id'] == df2['id'],\n",
    "    how='inner',\n",
    ")\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af898b40",
   "metadata": {},
   "source": [
    "This _seems_ fine initially, but spark blows up as soon as you try and use the 'id' column in an expression\n",
    "\n",
    "This example will produce the error:\n",
    "\n",
    "`[AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`].`\n",
    "\n",
    "This can be particularly annoying as the error will only appear when you attempt to use the columns, but will go undetected if this doesn't happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cab4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.utils\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import List\n",
    "\n",
    "def try_select(df: DataFrame, cols: List[str]):\n",
    "    try:\n",
    "        df.select(*cols).show()\n",
    "\n",
    "    except pyspark.sql.utils.AnalysisException as e:\n",
    "        print('select failed!', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f0c2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select failed! [AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`].\n"
     ]
    }
   ],
   "source": [
    "try_select(joined, ['id', 'name', 'trainer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d4744",
   "metadata": {},
   "source": [
    "The solution: use a different parameter for the `on` columns\n",
    "\n",
    "### 3.1.2 Join using list of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0bc54b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "| id|     name| name|\n",
      "+---+---------+-----+\n",
      "|007|charizard|  ash|\n",
      "|123|  pikachu|  ash|\n",
      "|999|     evee|chloe|\n",
      "+---+---------+-----+\n",
      "\n",
      "select failed! [AMBIGUOUS_REFERENCE] Reference `name` is ambiguous, could be: [`name`, `name`].\n"
     ]
    }
   ],
   "source": [
    "joined = df1.join(\n",
    "    df2,\n",
    "    on=['id'],\n",
    "    how='inner',\n",
    ")\n",
    "joined.show()\n",
    "\n",
    "# Now let's try that same select again\n",
    "try_select(joined, ['id', 'name', 'trainer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414bf5ac",
   "metadata": {},
   "source": [
    "### 3.1.3 Dataframe aliasing is a bit weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b46a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|123|  pikachu|\n",
      "|999|     evee|\n",
      "|007|charizard|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.alias('pokemon').select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccae01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+-----+\n",
      "| id|     name| id| name|\n",
      "+---+---------+---+-----+\n",
      "|007|charizard|007|  ash|\n",
      "|123|  pikachu|123|  ash|\n",
      "|999|     evee|999|chloe|\n",
      "+---+---------+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'id', 'name']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "joined = df1.alias('pokemon').join(\n",
    "    df2.alias('trainers'),\n",
    "    on=F.col('pokemon.id') == F.col('trainers.id'),\n",
    "    how='inner',\n",
    ")\n",
    "joined.show()\n",
    "joined.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19620ae6",
   "metadata": {},
   "source": [
    "Now, our error message is much better, as it contains the dataframe aliases identifying which table the duplicate column name is from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0a82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select failed! [AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`pokemon`.`id`, `trainers`.`id`].\n"
     ]
    }
   ],
   "source": [
    "try_select(joined, ['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d393943",
   "metadata": {},
   "source": [
    "Confusingly, using `Dataframe.columns` does not show the aliases, but they are usable when selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be334bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'id', 'name']\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|007|\n",
      "|123|\n",
      "|999|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(joined.columns)\n",
    "\n",
    "try_select(joined, ['pokemon.id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bdf9d-87c3-4347-9712-e49a22fea92c",
   "metadata": {},
   "source": [
    "## 4. Default empty DataFrames\n",
    "\n",
    "Sometimes it's handy to be able to instantiate an \"empty\" dataframe in the case that a file/some source data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f70151d6-b305-45ff-a680-81f3a7d0c3a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/articles/20230605_pyspark_fu/optional_source.json.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This will result in an AnalysisException complaining that \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# the file did not exist\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptional_source.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:418\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator: Iterable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/articles/20230605_pyspark_fu/optional_source.json."
     ]
    }
   ],
   "source": [
    "# This will result in an AnalysisException complaining that \n",
    "# the file did not exist\n",
    "\n",
    "df = spark.read.json('optional_source.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673e8c9-9bf9-47a1-9926-d3d94fdfcb57",
   "metadata": {},
   "source": [
    "We can mitigate this by catching the exception, and creating a dataframe that matches the schema, but has 0 rows.\n",
    "\n",
    "This ensures that any queries on the dataframe will still work, as all the columns will exist with the correct type.\n",
    "\n",
    "_**This requires that we know the schema of the optional file**_\n",
    "\n",
    "\n",
    "The easiest way to create a schema is usually to create a single-line file containing a valid line that matches the expected schema. Then, read that file into a dataframe and capture the schema for re-use (read: copy/paste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76f75b-ebe8-47d4-a14b-67ade411e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('not_there.json', 'w') as ostream:\n",
    "    ostream.write(json.dumps({\n",
    "        'id': 123, 'key': 'yolo', 'attrs': {'a': 'b'}\n",
    "    }))\n",
    "\n",
    "spark.read.json('not_there.json').schema.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55be1d-bf2b-4368-87c0-2a28974dc262",
   "metadata": {},
   "source": [
    "I've never found a way (using StringIO or similar) to achieve this without writing a file - if you find a way then let me know!\n",
    "\n",
    "Let's bundle this up into a method that tidies up after itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aca1da-147e-4ab4-ada4-c2642c066830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def guess_schema(row: dict, tmp_fpath: str = 'tmp.json') -> dict:\n",
    "    with open(tmp_fpath, 'w') as ostream:\n",
    "        ostream.write(json.dumps({\n",
    "            'id': 123, 'key': 'yolo', 'attrs': {'a': 'b'}\n",
    "        }))    \n",
    "    schema = json.loads(spark.read.json('not_there.json').schema.json())\n",
    "    os.remove(tmp_fpath)\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb184808-7c85-4506-ab9d-f7f2849eee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = guess_schema(\n",
    "    {'id': 123, 'key': 'yolo', 'attrs': {'a': 'b'}}\n",
    ")\n",
    "print(json.dumps(schema, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45033389-3134-42cb-aafd-2a6b38161cea",
   "metadata": {},
   "source": [
    "As you can see from this quick demo, it isn't quick to craft pyspark schemas from hand! In my experience it's prone to much human error and frustrating debugging, especially as schemas can grow large very quickly!\n",
    "\n",
    "Now, we can tie this into the method to safely load/create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05059828-375c-4a63-b8b4-766bcbf6568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.errors.exceptions.captured import AnalysisException\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "def safe_load(fpath: str, schema: dict):\n",
    "    try:\n",
    "        return spark.read.json(fpath)\n",
    "    except AnalysisException as e:\n",
    "        print(e)\n",
    "        return spark.createDataFrame([], schema=T.StructType.fromJson(schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b9778-c361-49d0-a111-b04858069fc6",
   "metadata": {},
   "source": [
    "> Side note: the method to convert a dict to a StructType (schema) is confusingly named `fromJson` despite the fact that the method accepts a dict, not a JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81a746-f9b6-4ed7-b34f-8e0c0375b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = safe_load('not_there.json', schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649aefc-7d18-48fe-9022-cbd0296bda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce755e33-0f8e-4fd8-8e52-d6650fd23f78",
   "metadata": {},
   "source": [
    "After the initial generation, the schema can be stored in a file and loaded or just defined directly in the code, rather than \"guessed\" every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f7696-c9d9-4b76-8f97-36f95e4f6bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
